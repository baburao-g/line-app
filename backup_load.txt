/usr/hdp/2.5.3.0-37/spark2/bin/spark-shell --driver-memory 8g --jars "/data/Prod/Demandplanning/jars/jsr166e-1.1.0.jar","/data/Prod/Demandplanning/jars/spark-cassandra-connector-2.0.0-M2-s_2.11.jar","/data/Prod/Demandplanning/jars/memsql-connector_2.11-2.0.4.jar","/data/Prod/Demandplanning/jars/mysql-connector-java-5.1.34.jar","/data/Prod/Demandplanning/jars/commons-dbcp2-2.1.1.jar","/data/Prod/Demandplanning/jars/commons-pool2-2.4.2.jar","/data/Prod/Demandplanning/jars/spray-json_2.11-1.3.3.jar" --conf spark.cassandra.connection.host=" " --conf spark.cassandra.auth.username="" --conf spark.cassandra.auth.password="" --conf spark.memsql.host="10.164.131.179" --conf spark.memsql.username="root" --conf spark.memsql.password="" --conf spark.memsql.port=3306 --conf spark.memsql.defaultDatabase="demandplanning_test"


/usr/hdp/2.5.3.0-37/spark2/bin/spark-shell --driver-memory 8g --jars "/data/Prod/Demandplanning/jars/jsr166e-1.1.0.jar","/data/Prod/Demandplanning/jars/spark-cassandra-connector-2.0.0-M2-s_2.11.jar","/data/Prod/Demandplanning/jars/memsql-connector_2.11-2.0.4.jar","/data/Prod/Demandplanning/jars/mysql-connector-java-5.1.34.jar","/data/Prod/Demandplanning/jars/commons-dbcp2-2.1.1.jar","/data/Prod/Demandplanning/jars/commons-pool2-2.4.2.jar","/data/Prod/Demandplanning/jars/spray-json_2.11-1.3.3.jar" --conf spark.cassandra.connection.host=" " --conf spark.cassandra.auth.username="" --conf spark.cassandra.auth.password="" --conf spark.memsql.host="10.164.131.143" --conf spark.memsql.username="root" --conf spark.memsql.password="" --conf spark.memsql.port=3306 --conf spark.memsql.defaultDatabase="demandplanning_test"


import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.DataFrame
import com.memsql.spark.connector.sql._
import com.memsql.spark.connector._
import com.memsql.spark.connector.rdd._
import com.memsql.spark.connector.dataframe._
import org.apache.spark.sql.cassandra._
import org.apache.spark.sql.SaveMode
import java.sql.Timestamp
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)




var df=spark.read.parquet("file:/data1/Testing/Backup/memsql_backup/demandplanning/master_salesitem_maintenance")
df1=df1.drop("id","order")
df.write.format("com.memsql.spark.connector").mode(SaveMode.Overwrite).save("master_salesitem_maintenance")

var df1=spark.read.parquet("file:/data1/Testing/Backup/memsql_backup/demandplanning/product_hierarchy")
df1=df1.drop("id","order")
df1.write.format("com.memsql.spark.connector").mode(SaveMode.Append).save("product_hierarchy")


var df2=spark.read.parquet("file:/data1/Testing/Backup/memsql_backup/demandplanning/data_change_log")
df1=df1.drop("id","order")
df2.write.format("com.memsql.spark.connector").mode(SaveMode.Overwrite).save("data_change_log")





